{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TDA_CoverDetection_Data_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2eZdho5C9yCNny078Nhkn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanteur/TDA_Cover_detection/blob/main/TDA_CoverDetection_Data_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYakvY2Zwu7d",
        "outputId": "bffac1aa-2e89-46fc-d334-5301bfd38c02"
      },
      "source": [
        "!pip install gudhi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gudhi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/02/84538da083305b7634886149331150399d6b5d9e4043852e4bfee3256468/gudhi-3.4.1.post1-cp37-cp37m-manylinux2014_x86_64.whl (28.2MB)\n",
            "\u001b[K     |████████████████████████████████| 28.2MB 152kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.7/dist-packages (from gudhi) (1.19.5)\n",
            "Installing collected packages: gudhi\n",
            "Successfully installed gudhi-3.4.1.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc-DrwZL4yFL"
      },
      "source": [
        "#Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eXrdsGn35K6"
      },
      "source": [
        "import gudhi as gd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gudhi.representations\n",
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdjwsALC3uz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de89ae3b-3405-4393-dcb1-d5437f1ca94a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiushyHj42DB"
      },
      "source": [
        "#Extracting data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws9-l7es8mnn"
      },
      "source": [
        "Getting a dataframe, which consists of name aka clique of songs and its cloud points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMZ5Z4Ke4xd0"
      },
      "source": [
        "def get_data(cloud_points_type, short_size_datasets=False):\n",
        "\n",
        "  input_dir = \"/content/drive/MyDrive/Colab Notebooks/CourseWork/cloud_points_dataset/cloud_points_\" + \\\n",
        "              cloud_points_type + '/'\n",
        "  #make dict\n",
        "  songs_dict = {}\n",
        "  i = 0\n",
        "  if short_size_datasets:\n",
        "    root_dir = input_dir + '_dur_60__off_10'\n",
        "  else:\n",
        "    root_dir = input_dir + '_dur_None__off_0'\n",
        "  for address, dirs, files in os.walk(root_dir):\n",
        "    if address == input_dir:\n",
        "      continue\n",
        "    for file in files:\n",
        "      songs_dict.setdefault(i,{})\n",
        "      data = np.loadtxt(address + '/' + file, delimiter=' ')\n",
        "      songs_dict[i]['name'] = address.rsplit('/', 1)[1]\n",
        "      songs_dict[i]['data'] = data.T\n",
        "      i += 1\n",
        "    \n",
        "  #make DataFrame\n",
        "  songs_df = pd.DataFrame(songs_dict)\n",
        "  return songs_df.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1nqfcq-6HIY"
      },
      "source": [
        "#all features cloud points full length songs\n",
        "df_af = get_data('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adeekt4i765i"
      },
      "source": [
        "#tonnetz cloud points full length songs\n",
        "df_tf = get_data('tonnetz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyLJcMWT78fs"
      },
      "source": [
        "#mfcc cloud points full length songs\n",
        "df_mf = get_data('mfcc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUHUftqz7-8c"
      },
      "source": [
        "#all features cloud points 60s length songs\n",
        "df_as = get_data('all', short_size_datasets=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4ASlFzU8Dzp"
      },
      "source": [
        "#tonnetz cloud points 60s length songs\n",
        "df_ts = get_data('tonnetz', short_size_datasets=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e-2NSqa8GEo"
      },
      "source": [
        "#mfcc cloud points 60s length songs\n",
        "df_ms = get_data('mfcc', short_size_datasets=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agpGc3ff5yj_"
      },
      "source": [
        "Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "0n-ei2xk6O7I",
        "outputId": "37ba4cd9-39c8-4a70-bd19-50ee49b6cef3"
      },
      "source": [
        "df_mf.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>All_Tomorrow_s_Parties</td>\n",
              "      <td>[[-3.103133201599121, 1.4269647598266602, 0.04...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>All_Tomorrow_s_Parties</td>\n",
              "      <td>[[-3.2100024223327637, 0.8784309029579163, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Addicted_To_Love</td>\n",
              "      <td>[[-3.0177416801452637, 1.3071744441986084, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Addicted_To_Love</td>\n",
              "      <td>[[-2.3204026222229004, 1.7416863441467285, -1....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>All_Along_The_Watchtower</td>\n",
              "      <td>[[-2.4056568145751953, 1.6976637840270996, -1....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       name                                               data\n",
              "0    All_Tomorrow_s_Parties  [[-3.103133201599121, 1.4269647598266602, 0.04...\n",
              "1    All_Tomorrow_s_Parties  [[-3.2100024223327637, 0.8784309029579163, 0.0...\n",
              "2          Addicted_To_Love  [[-3.0177416801452637, 1.3071744441986084, -0....\n",
              "3          Addicted_To_Love  [[-2.3204026222229004, 1.7416863441467285, -1....\n",
              "4  All_Along_The_Watchtower  [[-2.4056568145751953, 1.6976637840270996, -1...."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdA1mGHqFKln",
        "outputId": "00b66266-a856-40ab-95e3-fa80968fd699"
      },
      "source": [
        "df_af.iloc[159].data.shape, df_tf.iloc[159].data.shape, df_mf.iloc[159].data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((286, 30), (188, 6), (347, 12))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4DI46y36SP_",
        "outputId": "bae02ca0-97ab-4f31-e473-f1c51d677619"
      },
      "source": [
        "df_as.iloc[159].data.shape, df_ts.iloc[159].data.shape, df_ms.iloc[159].data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((58, 30), (58, 6), (58, 12))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXfUUKVcXV7O"
      },
      "source": [
        "#Reduce data\n",
        "Reducing data from full audios via PCA and choosing n farthest points **only for full audios**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL_Iaq4JXPEt"
      },
      "source": [
        "import gudhi.subsampling as gds\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def reduce_data(df):\n",
        "  df_size = df.shape[0]\n",
        "  pca = PCA(n_components=2)\n",
        "  feature_dict = {}\n",
        "  for index, row in df.iterrows():\n",
        "    if (index + 1) % 40 == 0:\n",
        "      print('Progress: {}/{}'.format(index+1,df_size))\n",
        "    song_data = np.array(row['data'])\n",
        "    sparse_song_data = np.array(gds.choose_n_farthest_points(song_data, nb_points=120))\n",
        "    sparse_song_data_pca = pca.fit_transform(sparse_song_data)\n",
        "    # print(sparse_song_data.shape, pca.explained_variance_ratio_)\n",
        "    # plt.scatter(sparse_song_data[:,0], sparse_song_data[:,1])\n",
        "    # plt.show()\n",
        "    feature_dict.setdefault(index, {})\n",
        "    feature_dict[index]['sparse_data'] = sparse_song_data_pca\n",
        "  dict_df = pd.DataFrame.from_dict(feature_dict).transpose()\n",
        "  joined_df = df.join(dict_df)\n",
        "  print('Done!\\n')\n",
        "  return joined_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHP0gR83XNYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3f34cf-8bd5-418f-da80-4bc692e66bfe"
      },
      "source": [
        "df_af = reduce_data(df_af)\n",
        "df_tf = reduce_data(df_tf)\n",
        "df_mf = reduce_data(df_mf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: 40/164\n",
            "Progress: 80/164\n",
            "Progress: 120/164\n",
            "Progress: 160/164\n",
            "Done!\n",
            "\n",
            "Progress: 40/164\n",
            "Progress: 80/164\n",
            "Progress: 120/164\n",
            "Progress: 160/164\n",
            "Done!\n",
            "\n",
            "Progress: 40/164\n",
            "Progress: 80/164\n",
            "Progress: 120/164\n",
            "Progress: 160/164\n",
            "Done!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBcRFpLO52Te"
      },
      "source": [
        "Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "rVuTRx4B6OI-",
        "outputId": "52e22432-2e96-48a6-948d-8d19003005e6"
      },
      "source": [
        "df_af.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>data</th>\n",
              "      <th>sparse_data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blue_Collar_Man</td>\n",
              "      <td>[[-4.582876612859176, 1.6642811155546193, -1.7...</td>\n",
              "      <td>[[1.8349494300835367, -0.37629420484752213], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Blue_Collar_Man</td>\n",
              "      <td>[[-4.508941001611552, 1.9314511059462562, -1.4...</td>\n",
              "      <td>[[-0.209837885003587, -0.22554664053336668], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Addicted_To_Love</td>\n",
              "      <td>[[-4.890550431132267, 1.8891033676869158, -0.4...</td>\n",
              "      <td>[[1.3351606727019654, -0.18788883166044545], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Addicted_To_Love</td>\n",
              "      <td>[[-3.7661504426205292, 2.629159154705121, -1.6...</td>\n",
              "      <td>[[0.16023083771694593, -0.42337137601792635], ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>All_Tomorrow_s_Parties</td>\n",
              "      <td>[[-5.055543636065286, 1.9339179479153203, -0.2...</td>\n",
              "      <td>[[0.27576475672799483, -0.03030909073657632], ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     name  ...                                        sparse_data\n",
              "0         Blue_Collar_Man  ...  [[1.8349494300835367, -0.37629420484752213], [...\n",
              "1         Blue_Collar_Man  ...  [[-0.209837885003587, -0.22554664053336668], [...\n",
              "2        Addicted_To_Love  ...  [[1.3351606727019654, -0.18788883166044545], [...\n",
              "3        Addicted_To_Love  ...  [[0.16023083771694593, -0.42337137601792635], ...\n",
              "4  All_Tomorrow_s_Parties  ...  [[0.27576475672799483, -0.03030909073657632], ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_-cKpWN82XH",
        "outputId": "6e827a4a-82be-494f-a2e5-5d2ab7c77e81"
      },
      "source": [
        "df_af.iloc[159].sparse_data.shape, df_tf.iloc[159].sparse_data.shape, df_mf.iloc[159].sparse_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((120, 2), (120, 2), (120, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSuL4PRNp3uc"
      },
      "source": [
        "#Optimization\n",
        "**Only tonnetz!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOiJreVgp3AD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3256a693-56de-49a2-fd06-bb1809749016"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP_oxZyZqVow"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHFplDhrqWuo"
      },
      "source": [
        "def Rips(DX, mel, dim, card):\n",
        "    # Parameters: DX (distance matrix), \n",
        "    #             mel (maximum edge length for Rips filtration), \n",
        "    #             dim (homological dimension), \n",
        "    #             card (number of persistence diagram points, sorted by distance-to-diagonal)\n",
        "\n",
        "    # Compute the persistence pairs with Gudhi\n",
        "    rc = gd.RipsComplex(distance_matrix=DX, max_edge_length=mel)\n",
        "    st = rc.create_simplex_tree(max_dimension=dim+1)\n",
        "    dgm = st.persistence()\n",
        "    pairs = st.persistence_pairs()\n",
        "\n",
        "    # Retrieve vertices v_a and v_b by picking the ones achieving the maximal\n",
        "    # distance among all pairwise distances between the simplex vertices\n",
        "    indices, pers = [], []\n",
        "    for s1, s2 in pairs:\n",
        "        if len(s1) == dim+1:\n",
        "            l1, l2 = np.array(s1), np.array(s2)\n",
        "            i1 = [s1[v] for v in np.unravel_index(np.argmax(DX[l1,:][:,l1]),[len(s1), len(s1)])]\n",
        "            i2 = [s2[v] for v in np.unravel_index(np.argmax(DX[l2,:][:,l2]),[len(s2), len(s2)])]\n",
        "            indices += i1\n",
        "            indices += i2\n",
        "            pers.append(st.filtration(s2) - st.filtration(s1))\n",
        "    \n",
        "    # Sort points with distance-to-diagonal\n",
        "    perm = np.argsort(pers)\n",
        "    indices = list(np.reshape(indices, [-1,4])[perm][::-1,:].flatten())\n",
        "    \n",
        "    # Output indices\n",
        "    indices = indices[:4*card] + [0 for _ in range(0,max(0,4*card-len(indices)))]\n",
        "    return list(np.array(indices, dtype=np.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeC98BkOqYhw"
      },
      "source": [
        "class RipsModel(tf.keras.Model):\n",
        "    def __init__(self, X, mel=12, dim=1, card=50):\n",
        "        super(RipsModel, self).__init__()\n",
        "        self.X = X\n",
        "        self.mel = mel\n",
        "        self.dim = dim\n",
        "        self.card = card\n",
        "        \n",
        "    def call(self):\n",
        "        m, d, c = self.mel, self.dim, self.card\n",
        "        \n",
        "        # Compute distance matrix\n",
        "        DX = tfa.losses.metric_learning.pairwise_distance(self.X)\n",
        "        DXX = tf.reshape(DX, [1, DX.shape[0], DX.shape[1]])\n",
        "        \n",
        "        # Turn numpy function into tensorflow function\n",
        "        RipsTF = lambda DX: tf.numpy_function(Rips, [DX, m, d, c], [tf.int32 for _ in range(4*c)])\n",
        "        \n",
        "        # Compute vertices associated to positive and negative simplices \n",
        "        # Don't compute gradient for this operation\n",
        "        ids = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(RipsTF,DXX,dtype=[tf.int32 for _ in range(4*c)]))\n",
        "        \n",
        "        # Get persistence diagram by simply picking the corresponding entries in the distance matrix\n",
        "        dgm = tf.reshape(tf.gather_nd(DX, tf.reshape(ids, [2*c,2])), [c,2])\n",
        "        return dgm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrzxzci1spGR"
      },
      "source": [
        "def optimize_point_clouds(df, column, n_pts=300, card=50, hom=1, ml=12., n_epochs=40):\n",
        "  \"\"\"\n",
        "\n",
        "  params:\n",
        "    df : pd.DataFrame\n",
        "      initial dataframe\n",
        "    column : str\n",
        "      name of data column\n",
        "    n_pts : int\n",
        "      number of points in the point clouds\n",
        "    card  : int\n",
        "      max number of points in the diagrams\n",
        "    hom   : int\n",
        "      homological dimension\n",
        "    ml    : int\n",
        "      max distance in Rips\n",
        "    n_epochs : int\n",
        "      number of optimization steps\n",
        "\n",
        "  returns:\n",
        "    pd.Dataframe with added column of optimized cloud points\n",
        "  \"\"\"\n",
        "\n",
        "  feature_dict = {}\n",
        "  df_size = df.shape[0]\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    if (index + 1) % 20 == 0:\n",
        "      print('Progress: {}/{}'.format(index+1,df_size))\n",
        "\n",
        "    song_data = row[column].astype(np.float32)\n",
        "    \n",
        "    #tensorflow part\n",
        "    X = tf.Variable(initial_value=song_data, trainable=True)\n",
        "    model = RipsModel(X=X, mel=ml, dim=hom, card=card)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "\n",
        "    for epoch in range(n_epochs+1):\n",
        "    \n",
        "      with tf.GradientTape() as tape:\n",
        "        # Compute persistence diagram\n",
        "        dgm = model.call()\n",
        "        \n",
        "        # Loss is sum of squares of distances to the diagonal\n",
        "        loss = -tf.math.reduce_sum(tf.square(.5*(dgm[:,1]-dgm[:,0])))\n",
        "          \n",
        "      # Compute and apply gradients\n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    # add to dict\n",
        "    feature_dict.setdefault(index, {})\n",
        "    feature_dict[index][column + '_optimized'] = model.X.numpy()\n",
        "\n",
        "  dict_df = pd.DataFrame.from_dict(feature_dict).transpose()\n",
        "  joined_df = df.join(dict_df)\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "  return joined_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz8rP8KwuFvu"
      },
      "source": [
        "df_ts = optimize_point_clouds(df_ts, column='data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhrD1W-YFASs",
        "outputId": "9f70b52a-53fc-40ee-ed85-e149a33d2a90"
      },
      "source": [
        "df_tf = optimize_point_clouds(df_tf, column='sparse_data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: 20/164\n",
            "Progress: 40/164\n",
            "Progress: 60/164\n",
            "Progress: 80/164\n",
            "Progress: 100/164\n",
            "Progress: 120/164\n",
            "Progress: 140/164\n",
            "Progress: 160/164\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "fbs0DttpzS95",
        "outputId": "7744b5c1-8059-4520-fa4a-0eae152fe6b5"
      },
      "source": [
        "df_af.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>data</th>\n",
              "      <th>sparse_data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blue_Collar_Man</td>\n",
              "      <td>[[-4.582876612859176, 1.6642811155546193, -1.7...</td>\n",
              "      <td>[[1.8349494300835367, -0.37629420484752213], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Blue_Collar_Man</td>\n",
              "      <td>[[-4.508941001611552, 1.9314511059462562, -1.4...</td>\n",
              "      <td>[[-0.209837885003587, -0.22554664053336668], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Addicted_To_Love</td>\n",
              "      <td>[[-4.890550431132267, 1.8891033676869158, -0.4...</td>\n",
              "      <td>[[1.3351606727019654, -0.18788883166044545], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Addicted_To_Love</td>\n",
              "      <td>[[-3.7661504426205292, 2.629159154705121, -1.6...</td>\n",
              "      <td>[[0.16023083771694593, -0.42337137601792635], ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>All_Tomorrow_s_Parties</td>\n",
              "      <td>[[-5.055543636065286, 1.9339179479153203, -0.2...</td>\n",
              "      <td>[[0.27576475672799483, -0.03030909073657632], ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     name  ...                                        sparse_data\n",
              "0         Blue_Collar_Man  ...  [[1.8349494300835367, -0.37629420484752213], [...\n",
              "1         Blue_Collar_Man  ...  [[-0.209837885003587, -0.22554664053336668], [...\n",
              "2        Addicted_To_Love  ...  [[1.3351606727019654, -0.18788883166044545], [...\n",
              "3        Addicted_To_Love  ...  [[0.16023083771694593, -0.42337137601792635], ...\n",
              "4  All_Tomorrow_s_Parties  ...  [[0.27576475672799483, -0.03030909073657632], ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkOoL_Jb9URq"
      },
      "source": [
        "#Saving datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOv0RAXE9byz"
      },
      "source": [
        "root_dir = '/content/drive/MyDrive/Colab Notebooks/CourseWork/data_datasets/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai-J7vye9TtB"
      },
      "source": [
        "df_as.to_json(root_dir + 'data_all_short.json')\n",
        "df_ts.to_json(root_dir + 'data_tonnetz_short.json')\n",
        "df_ms.to_json(root_dir + 'data_mfcc_short.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AZ5ggbPFVRX"
      },
      "source": [
        "df_af.to_json(root_dir + 'data_all_full.json')\n",
        "df_tf.to_json(root_dir + 'data_tonnetz_full.json')\n",
        "df_mf.to_json(root_dir + 'data_mfcc_full.json')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
